# Data Lab ðŸ’¾

# Practice #2 - ETL with Apache-spark

### Objective:

Similar to the practice #1, consume from `azure-blob-storage` and store the data into a `synapse` table, but now using `Apache-spark` (with `Python`)

__Details:__ 

1. Now, using different services, weâ€™ll consume same data, but store it into `synapse` storage. 
2. Try using `spark-notebooks` for data exploration and once the logic is ready, create a `spark-
batch-job` (adhoc execution - not scheduled) which will perform the ETL logic. 
3. For the output, feel free to define the schema as you consider, also for the data manipulation

__Extras:__
- Try using another dataset, different format with multiple files 
- Try different transformations to manipulate data
- Try different output options: format, save modes, etc
